[2025-11-15 14:00:58] train_geo_csf.py(180) : DEBUG_MODE: False
[2025-11-15 14:00:58] train_geo_csf.py(182) : USE_CUDA: True, CUDA_DEVICE_NUM: 0
[2025-11-15 14:00:58] train_geo_csf.py(184) : env_params{'weca_model_params': {'embedding_dim': 128, 'sqrt_embedding_dim': 11.313708498984761, 'encoder_layer_num': 6, 'qkv_dim': 16, 'head_num': 8, 'logit_clipping': 10, 'ff_hidden_dim': 512, 'eval_type': 'argmax', 'hyper_hidden_dim': 256}, 'weca_env_params': {'problem_size': 20, 'pomo_size': 20}, 'weca_checkpoint_path': './POMO/result/train__tsp_n20/checkpoint_motsp-200.pt', 'ref_point': [20.0, 20.0]}
[2025-11-15 14:00:58] train_geo_csf.py(184) : actor_params{'gfp_params': {'input_dim': 128, 'hidden_dim': 128, 'output_dim': 128, 'num_layers': 2, 'num_heads': 8, 'ff_hidden_dim': 512}, 'csf_params': {'input_dim': 2, 'hidden_dim': 128, 'condition_dim': 128, 'geometric_dim': 128, 'time_embed_dim': 128, 'num_layers': 2, 'num_heads': 8, 'ff_hidden_dim': 512}, 'N': 10, 'M': 2}
[2025-11-15 14:00:58] train_geo_csf.py(184) : critic_params{'node_embedding_dim': 128, 'N': 10, 'M': 2, 'hidden_dim': 128, 'geometric_dim': 128}
[2025-11-15 14:00:58] train_geo_csf.py(184) : optimizer_params{'lr_actor': 1e-05, 'lr_critic': 0.0001}
[2025-11-15 14:00:58] train_geo_csf.py(184) : trainer_params{'use_cuda': True, 'cuda_device_num': 0, 'device': device(type='cuda', index=0), 'num_episodes': 10000, 'batch_size': 256, 'buffer_capacity': 100000, 'start_train_after_episodes': 100, 'max_grad_norm_critic': 1.0, 'logging': {'model_save_interval': 5, 'img_save_interval': 10, 'log_image_params_1': {'json_foldername': 'log_image_style', 'filename': 'style_tsp_20.json'}, 'log_image_params_2': {'json_foldername': 'log_image_style', 'filename': 'style_loss_1.json'}}, 'model_load': {'enable': False, 'path': './result/saved_geo_csf_model', 'stage_to_load': 0}}
[2025-11-15 14:00:58] train_geo_csf.py(184) : logger_params{'log_file': {'desc': 'train_geo_csf', 'filename': 'run_log', 'filepath': './result/20251115_140058_train_geo_csf'}}
[2025-11-15 14:00:59] Trainer.py(107) : --- 开始单阶段训练: N = 10, Episodes = 10000 ---
[2025-11-15 14:04:31] Trainer.py(180) : Episode 100: 平均奖励=0.6124
[2025-11-15 14:04:31] Trainer.py(181) :   损失 (Actor): 0.0000
[2025-11-15 14:04:31] Trainer.py(182) :   损失 (Critic, 总加权): 0.0034
[2025-11-15 14:04:31] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:04:31] Trainer.py(184) :   未加权 L_hv: 0.003366
[2025-11-15 14:04:31] Trainer.py(185) :   -------------------
[2025-11-15 14:04:31] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:04:31] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:04:31] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6124 *** 
[2025-11-15 14:08:00] Trainer.py(180) : Episode 200: 平均奖励=0.6131
[2025-11-15 14:08:00] Trainer.py(181) :   损失 (Actor): -0.5876
[2025-11-15 14:08:00] Trainer.py(182) :   损失 (Critic, 总加权): 0.0085
[2025-11-15 14:08:00] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:08:00] Trainer.py(184) :   未加权 L_hv: 0.008547
[2025-11-15 14:08:00] Trainer.py(185) :   -------------------
[2025-11-15 14:08:00] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:08:00] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:08:00] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6131 *** 
[2025-11-15 14:11:24] Trainer.py(180) : Episode 300: 平均奖励=0.6091
[2025-11-15 14:11:24] Trainer.py(181) :   损失 (Actor): -0.6250
[2025-11-15 14:11:24] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:11:24] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:11:24] Trainer.py(184) :   未加权 L_hv: 0.000524
[2025-11-15 14:11:24] Trainer.py(185) :   -------------------
[2025-11-15 14:14:55] Trainer.py(180) : Episode 400: 平均奖励=0.6114
[2025-11-15 14:14:55] Trainer.py(181) :   损失 (Actor): -0.6051
[2025-11-15 14:14:55] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:14:55] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:14:55] Trainer.py(184) :   未加权 L_hv: 0.000513
[2025-11-15 14:14:55] Trainer.py(185) :   -------------------
[2025-11-15 14:18:43] Trainer.py(180) : Episode 500: 平均奖励=0.6158
[2025-11-15 14:18:43] Trainer.py(181) :   损失 (Actor): -0.6091
[2025-11-15 14:18:43] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:18:43] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:18:43] Trainer.py(184) :   未加权 L_hv: 0.000479
[2025-11-15 14:18:43] Trainer.py(185) :   -------------------
[2025-11-15 14:18:43] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:18:43] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:18:43] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6158 *** 
[2025-11-15 14:22:15] Trainer.py(180) : Episode 600: 平均奖励=0.6133
[2025-11-15 14:22:15] Trainer.py(181) :   损失 (Actor): -0.6299
[2025-11-15 14:22:15] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:22:15] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:22:15] Trainer.py(184) :   未加权 L_hv: 0.000481
[2025-11-15 14:22:15] Trainer.py(185) :   -------------------
[2025-11-15 14:25:49] Trainer.py(180) : Episode 700: 平均奖励=0.6089
[2025-11-15 14:25:49] Trainer.py(181) :   损失 (Actor): -0.6051
[2025-11-15 14:25:49] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:25:49] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:25:49] Trainer.py(184) :   未加权 L_hv: 0.000515
[2025-11-15 14:25:49] Trainer.py(185) :   -------------------
[2025-11-15 14:29:31] Trainer.py(180) : Episode 800: 平均奖励=0.6129
[2025-11-15 14:29:31] Trainer.py(181) :   损失 (Actor): -0.6056
[2025-11-15 14:29:31] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:29:31] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:29:31] Trainer.py(184) :   未加权 L_hv: 0.000514
[2025-11-15 14:29:31] Trainer.py(185) :   -------------------
[2025-11-15 14:33:13] Trainer.py(180) : Episode 900: 平均奖励=0.6164
[2025-11-15 14:33:13] Trainer.py(181) :   损失 (Actor): -0.6282
[2025-11-15 14:33:13] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:33:13] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:33:13] Trainer.py(184) :   未加权 L_hv: 0.000514
[2025-11-15 14:33:13] Trainer.py(185) :   -------------------
[2025-11-15 14:33:13] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:33:13] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:33:13] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6164 *** 
[2025-11-15 14:36:29] Trainer.py(180) : Episode 1000: 平均奖励=0.6151
[2025-11-15 14:36:29] Trainer.py(181) :   损失 (Actor): -0.6099
[2025-11-15 14:36:29] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:36:29] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:36:29] Trainer.py(184) :   未加权 L_hv: 0.000494
[2025-11-15 14:36:29] Trainer.py(185) :   -------------------
[2025-11-15 14:39:48] Trainer.py(180) : Episode 1100: 平均奖励=0.6144
[2025-11-15 14:39:48] Trainer.py(181) :   损失 (Actor): -0.6082
[2025-11-15 14:39:48] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 14:39:48] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:39:48] Trainer.py(184) :   未加权 L_hv: 0.000464
[2025-11-15 14:39:48] Trainer.py(185) :   -------------------
[2025-11-15 14:43:17] Trainer.py(180) : Episode 1200: 平均奖励=0.6160
[2025-11-15 14:43:17] Trainer.py(181) :   损失 (Actor): -0.6262
[2025-11-15 14:43:17] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 14:43:17] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:43:17] Trainer.py(184) :   未加权 L_hv: 0.000423
[2025-11-15 14:43:17] Trainer.py(185) :   -------------------
[2025-11-15 14:46:38] Trainer.py(180) : Episode 1300: 平均奖励=0.6169
[2025-11-15 14:46:38] Trainer.py(181) :   损失 (Actor): -0.6093
[2025-11-15 14:46:38] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 14:46:38] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:46:38] Trainer.py(184) :   未加权 L_hv: 0.000421
[2025-11-15 14:46:38] Trainer.py(185) :   -------------------
[2025-11-15 14:46:38] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:46:38] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:46:38] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6169 *** 
[2025-11-15 14:50:00] Trainer.py(180) : Episode 1400: 平均奖励=0.6170
[2025-11-15 14:50:00] Trainer.py(181) :   损失 (Actor): -0.6099
[2025-11-15 14:50:00] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 14:50:00] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:50:00] Trainer.py(184) :   未加权 L_hv: 0.000417
[2025-11-15 14:50:00] Trainer.py(185) :   -------------------
[2025-11-15 14:50:00] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:50:01] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:50:01] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6170 *** 
[2025-11-15 14:53:22] Trainer.py(180) : Episode 1500: 平均奖励=0.6171
[2025-11-15 14:53:22] Trainer.py(181) :   损失 (Actor): -0.6280
[2025-11-15 14:53:22] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 14:53:22] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:53:22] Trainer.py(184) :   未加权 L_hv: 0.000415
[2025-11-15 14:53:22] Trainer.py(185) :   -------------------
[2025-11-15 14:53:22] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:53:22] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:53:22] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6171 *** 
[2025-11-15 14:56:45] Trainer.py(180) : Episode 1600: 平均奖励=0.6173
[2025-11-15 14:56:45] Trainer.py(181) :   损失 (Actor): -0.6094
[2025-11-15 14:56:45] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 14:56:45] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 14:56:45] Trainer.py(184) :   未加权 L_hv: 0.000399
[2025-11-15 14:56:45] Trainer.py(185) :   -------------------
[2025-11-15 14:56:45] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 14:56:45] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 14:56:45] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6173 *** 
[2025-11-15 15:00:06] Trainer.py(180) : Episode 1700: 平均奖励=0.6174
[2025-11-15 15:00:06] Trainer.py(181) :   损失 (Actor): -0.6101
[2025-11-15 15:00:06] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:00:06] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:00:06] Trainer.py(184) :   未加权 L_hv: 0.000409
[2025-11-15 15:00:06] Trainer.py(185) :   -------------------
[2025-11-15 15:00:06] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:00:06] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:00:06] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6174 *** 
[2025-11-15 15:03:30] Trainer.py(180) : Episode 1800: 平均奖励=0.6176
[2025-11-15 15:03:30] Trainer.py(181) :   损失 (Actor): -0.6288
[2025-11-15 15:03:30] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:03:30] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:03:30] Trainer.py(184) :   未加权 L_hv: 0.000409
[2025-11-15 15:03:30] Trainer.py(185) :   -------------------
[2025-11-15 15:03:30] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:03:31] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:03:31] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6176 *** 
[2025-11-15 15:06:57] Trainer.py(180) : Episode 1900: 平均奖励=0.6177
[2025-11-15 15:06:57] Trainer.py(181) :   损失 (Actor): -0.6109
[2025-11-15 15:06:57] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:06:57] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:06:57] Trainer.py(184) :   未加权 L_hv: 0.000404
[2025-11-15 15:06:57] Trainer.py(185) :   -------------------
[2025-11-15 15:06:57] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:06:57] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:06:57] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6177 *** 
[2025-11-15 15:10:21] Trainer.py(180) : Episode 2000: 平均奖励=0.6177
[2025-11-15 15:10:21] Trainer.py(181) :   损失 (Actor): -0.6108
[2025-11-15 15:10:21] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:10:21] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:10:21] Trainer.py(184) :   未加权 L_hv: 0.000396
[2025-11-15 15:10:21] Trainer.py(185) :   -------------------
[2025-11-15 15:10:21] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:10:21] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:10:21] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6177 *** 
[2025-11-15 15:13:44] Trainer.py(180) : Episode 2100: 平均奖励=0.6180
[2025-11-15 15:13:44] Trainer.py(181) :   损失 (Actor): -0.6294
[2025-11-15 15:13:44] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:13:44] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:13:44] Trainer.py(184) :   未加权 L_hv: 0.000405
[2025-11-15 15:13:44] Trainer.py(185) :   -------------------
[2025-11-15 15:13:44] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:13:45] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:13:45] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6180 *** 
[2025-11-15 15:17:08] Trainer.py(180) : Episode 2200: 平均奖励=0.6185
[2025-11-15 15:17:08] Trainer.py(181) :   损失 (Actor): -0.6115
[2025-11-15 15:17:08] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:17:08] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:17:08] Trainer.py(184) :   未加权 L_hv: 0.000400
[2025-11-15 15:17:08] Trainer.py(185) :   -------------------
[2025-11-15 15:17:08] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:17:08] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:17:08] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6185 *** 
[2025-11-15 15:20:30] Trainer.py(180) : Episode 2300: 平均奖励=0.6185
[2025-11-15 15:20:30] Trainer.py(181) :   损失 (Actor): -0.6113
[2025-11-15 15:20:30] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:20:30] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:20:30] Trainer.py(184) :   未加权 L_hv: 0.000399
[2025-11-15 15:20:30] Trainer.py(185) :   -------------------
[2025-11-15 15:23:52] Trainer.py(180) : Episode 2400: 平均奖励=0.6186
[2025-11-15 15:23:52] Trainer.py(181) :   损失 (Actor): -0.6295
[2025-11-15 15:23:52] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:23:52] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:23:52] Trainer.py(184) :   未加权 L_hv: 0.000385
[2025-11-15 15:23:52] Trainer.py(185) :   -------------------
[2025-11-15 15:23:52] Trainer.py(319) : --- 保存最佳模型 (N=10) ---
[2025-11-15 15:23:52] Trainer.py(336) : 模型已保存到: ./result/20251115_140058_train_geo_csf/model_N10_best.pth
[2025-11-15 15:23:52] Trainer.py(192) :  *** 保存新的最佳模型! 平均奖励: 0.6186 *** 
[2025-11-15 15:27:13] Trainer.py(180) : Episode 2500: 平均奖励=0.6178
[2025-11-15 15:27:13] Trainer.py(181) :   损失 (Actor): -0.6110
[2025-11-15 15:27:13] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:27:13] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:27:13] Trainer.py(184) :   未加权 L_hv: 0.000388
[2025-11-15 15:27:13] Trainer.py(185) :   -------------------
[2025-11-15 15:30:37] Trainer.py(180) : Episode 2600: 平均奖励=0.6146
[2025-11-15 15:30:37] Trainer.py(181) :   损失 (Actor): -0.6091
[2025-11-15 15:30:37] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:30:37] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:30:37] Trainer.py(184) :   未加权 L_hv: 0.000390
[2025-11-15 15:30:37] Trainer.py(185) :   -------------------
[2025-11-15 15:34:03] Trainer.py(180) : Episode 2700: 平均奖励=0.6039
[2025-11-15 15:34:03] Trainer.py(181) :   损失 (Actor): -0.6177
[2025-11-15 15:34:03] Trainer.py(182) :   损失 (Critic, 总加权): 0.0004
[2025-11-15 15:34:03] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:34:03] Trainer.py(184) :   未加权 L_hv: 0.000423
[2025-11-15 15:34:03] Trainer.py(185) :   -------------------
[2025-11-15 15:37:29] Trainer.py(180) : Episode 2800: 平均奖励=0.5874
[2025-11-15 15:37:29] Trainer.py(181) :   损失 (Actor): -0.5817
[2025-11-15 15:37:29] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 15:37:29] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:37:29] Trainer.py(184) :   未加权 L_hv: 0.000494
[2025-11-15 15:37:29] Trainer.py(185) :   -------------------
[2025-11-15 15:41:00] Trainer.py(180) : Episode 2900: 平均奖励=0.5780
[2025-11-15 15:41:00] Trainer.py(181) :   损失 (Actor): -0.5696
[2025-11-15 15:41:00] Trainer.py(182) :   损失 (Critic, 总加权): 0.0005
[2025-11-15 15:41:00] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:41:00] Trainer.py(184) :   未加权 L_hv: 0.000534
[2025-11-15 15:41:00] Trainer.py(185) :   -------------------
[2025-11-15 15:44:28] Trainer.py(180) : Episode 3000: 平均奖励=0.5721
[2025-11-15 15:44:28] Trainer.py(181) :   损失 (Actor): -0.5807
[2025-11-15 15:44:28] Trainer.py(182) :   损失 (Critic, 总加权): 0.0006
[2025-11-15 15:44:28] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:44:28] Trainer.py(184) :   未加权 L_hv: 0.000563
[2025-11-15 15:44:28] Trainer.py(185) :   -------------------
[2025-11-15 15:47:49] Trainer.py(180) : Episode 3100: 平均奖励=0.5676
[2025-11-15 15:47:49] Trainer.py(181) :   损失 (Actor): -0.5605
[2025-11-15 15:47:49] Trainer.py(182) :   损失 (Critic, 总加权): 0.0006
[2025-11-15 15:47:49] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:47:49] Trainer.py(184) :   未加权 L_hv: 0.000623
[2025-11-15 15:47:49] Trainer.py(185) :   -------------------
[2025-11-15 15:51:11] Trainer.py(180) : Episode 3200: 平均奖励=0.5613
[2025-11-15 15:51:11] Trainer.py(181) :   损失 (Actor): -0.5528
[2025-11-15 15:51:11] Trainer.py(182) :   损失 (Critic, 总加权): 0.0007
[2025-11-15 15:51:11] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:51:11] Trainer.py(184) :   未加权 L_hv: 0.000682
[2025-11-15 15:51:11] Trainer.py(185) :   -------------------
[2025-11-15 15:54:41] Trainer.py(180) : Episode 3300: 平均奖励=0.5571
[2025-11-15 15:54:41] Trainer.py(181) :   损失 (Actor): -0.5667
[2025-11-15 15:54:41] Trainer.py(182) :   损失 (Critic, 总加权): 0.0007
[2025-11-15 15:54:41] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:54:41] Trainer.py(184) :   未加权 L_hv: 0.000704
[2025-11-15 15:54:41] Trainer.py(185) :   -------------------
[2025-11-15 15:58:12] Trainer.py(180) : Episode 3400: 平均奖励=0.5519
[2025-11-15 15:58:12] Trainer.py(181) :   损失 (Actor): -0.5437
[2025-11-15 15:58:12] Trainer.py(182) :   损失 (Critic, 总加权): 0.0007
[2025-11-15 15:58:12] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 15:58:12] Trainer.py(184) :   未加权 L_hv: 0.000738
[2025-11-15 15:58:12] Trainer.py(185) :   -------------------
[2025-11-15 16:01:55] Trainer.py(180) : Episode 3500: 平均奖励=0.5468
[2025-11-15 16:01:55] Trainer.py(181) :   损失 (Actor): -0.5395
[2025-11-15 16:01:55] Trainer.py(182) :   损失 (Critic, 总加权): 0.0008
[2025-11-15 16:01:55] Trainer.py(183) :   --- 尺度监控 ---
[2025-11-15 16:01:55] Trainer.py(184) :   未加权 L_hv: 0.000762
[2025-11-15 16:01:55] Trainer.py(185) :   -------------------
