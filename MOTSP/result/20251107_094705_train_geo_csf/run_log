[2025-11-07 09:47:05] train_geo_csf.py(179) : DEBUG_MODE: False
[2025-11-07 09:47:05] train_geo_csf.py(181) : USE_CUDA: True, CUDA_DEVICE_NUM: 0
[2025-11-07 09:47:05] train_geo_csf.py(183) : env_params{'weca_model_params': {'embedding_dim': 128, 'sqrt_embedding_dim': 11.313708498984761, 'encoder_layer_num': 6, 'qkv_dim': 16, 'head_num': 8, 'logit_clipping': 10, 'ff_hidden_dim': 512, 'eval_type': 'argmax', 'hyper_hidden_dim': 256}, 'weca_env_params': {'problem_size': 20, 'pomo_size': 20}, 'weca_checkpoint_path': './POMO/result/train__tsp_n20/checkpoint_motsp-200.pt', 'ref_point': [20.0, 20.0]}
[2025-11-07 09:47:05] train_geo_csf.py(183) : actor_params{'gfp_params': {'input_dim': 128, 'hidden_dims': [64, 32], 'output_dim': 16, 'activation': 'relu'}, 'csf_params': {'input_dim': 2, 'hidden_dim': 128, 'condition_dim': 128, 'geometric_dim': 16, 'time_embed_dim': 128, 'num_layers': 2, 'num_heads': 8, 'ff_hidden_dim': 512}, 'N': 10, 'M': 2}
[2025-11-07 09:47:05] train_geo_csf.py(183) : critic_params{'condition_dim': 128, 'N': 10, 'M': 2, 'hidden_dim': 128}
[2025-11-07 09:47:05] train_geo_csf.py(183) : optimizer_params{'lr_actor': 1e-05, 'lr_critic': 0.0001}
[2025-11-07 09:47:05] train_geo_csf.py(183) : trainer_params{'use_cuda': True, 'cuda_device_num': 0, 'device': device(type='cuda', index=0), 'num_episodes': 5000, 'batch_size': 32, 'buffer_capacity': 50000, 'gamma': 0.99, 'tau': 0.005, 'noise_level': 0.1, 'train_steps_per_episode': 10, 'start_train_after_episodes': 100, 'logging': {'model_save_interval': 5, 'img_save_interval': 10, 'log_image_params_1': {'json_foldername': 'log_image_style', 'filename': 'style_tsp_20.json'}, 'log_image_params_2': {'json_foldername': 'log_image_style', 'filename': 'style_loss_1.json'}}, 'model_load': {'enable': False, 'path': './result/saved_geo_csf_model', 'stage_to_load': 0}}
[2025-11-07 09:47:05] train_geo_csf.py(183) : logger_params{'log_file': {'desc': 'train_geo_csf', 'filename': 'run_log', 'filepath': './result/20251107_094705_train_geo_csf'}}
[2025-11-07 09:47:06] Trainer.py(122) : --- 开始单阶段训练: N = 10, Episodes = 5000 ---
[2025-11-07 09:47:48] Trainer.py(184) : Episode 100: 平均奖励=0.6044, Actor损失=0.0000, Critic损失=0.0035
[2025-11-07 09:47:48] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 09:47:48] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 09:47:48] Trainer.py(192) : Episode 100: 保存了新的最佳模型，平均奖励=0.6044
[2025-11-07 09:48:36] Trainer.py(184) : Episode 200: 平均奖励=0.5829, Actor损失=-0.4310, Critic损失=0.0779
[2025-11-07 09:49:26] Trainer.py(184) : Episode 300: 平均奖励=0.4834, Actor损失=-0.5623, Critic损失=0.0029
[2025-11-07 09:50:15] Trainer.py(184) : Episode 400: 平均奖励=0.5770, Actor损失=-0.5765, Critic损失=0.0016
[2025-11-07 09:51:03] Trainer.py(184) : Episode 500: 平均奖励=0.5618, Actor损失=-0.6001, Critic损失=0.0017
[2025-11-07 09:51:52] Trainer.py(184) : Episode 600: 平均奖励=0.6104, Actor损失=-0.6065, Critic损失=0.0013
[2025-11-07 09:51:52] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 09:51:52] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 09:51:52] Trainer.py(192) : Episode 600: 保存了新的最佳模型，平均奖励=0.6104
[2025-11-07 09:52:41] Trainer.py(184) : Episode 700: 平均奖励=0.6144, Actor损失=-0.6193, Critic损失=0.0012
[2025-11-07 09:52:41] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 09:52:41] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 09:52:41] Trainer.py(192) : Episode 700: 保存了新的最佳模型，平均奖励=0.6144
[2025-11-07 09:53:27] Trainer.py(184) : Episode 800: 平均奖励=0.6142, Actor损失=-0.6213, Critic损失=0.0011
[2025-11-07 09:54:12] Trainer.py(184) : Episode 900: 平均奖励=0.6101, Actor损失=-0.6220, Critic损失=0.0010
[2025-11-07 09:54:57] Trainer.py(184) : Episode 1000: 平均奖励=0.6059, Actor损失=-0.6185, Critic损失=0.0010
[2025-11-07 09:55:43] Trainer.py(184) : Episode 1100: 平均奖励=0.6046, Actor损失=-0.6138, Critic损失=0.0010
[2025-11-07 09:56:30] Trainer.py(184) : Episode 1200: 平均奖励=0.6068, Actor损失=-0.6129, Critic损失=0.0009
[2025-11-07 09:57:26] Trainer.py(184) : Episode 1300: 平均奖励=0.6106, Actor损失=-0.6108, Critic损失=0.0009
[2025-11-07 09:58:17] Trainer.py(184) : Episode 1400: 平均奖励=0.6138, Actor损失=-0.6124, Critic损失=0.0009
[2025-11-07 09:59:06] Trainer.py(184) : Episode 1500: 平均奖励=0.6155, Actor损失=-0.6124, Critic损失=0.0009
[2025-11-07 09:59:06] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 09:59:06] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 09:59:06] Trainer.py(192) : Episode 1500: 保存了新的最佳模型，平均奖励=0.6155
[2025-11-07 09:59:59] Trainer.py(184) : Episode 1600: 平均奖励=0.6167, Actor损失=-0.6150, Critic损失=0.0008
[2025-11-07 09:59:59] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 09:59:59] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 09:59:59] Trainer.py(192) : Episode 1600: 保存了新的最佳模型，平均奖励=0.6167
[2025-11-07 10:00:54] Trainer.py(184) : Episode 1700: 平均奖励=0.6161, Actor损失=-0.6157, Critic损失=0.0008
[2025-11-07 10:01:49] Trainer.py(184) : Episode 1800: 平均奖励=0.6159, Actor损失=-0.6161, Critic损失=0.0008
[2025-11-07 10:02:44] Trainer.py(184) : Episode 1900: 平均奖励=0.6160, Actor损失=-0.6173, Critic损失=0.0007
[2025-11-07 10:03:33] Trainer.py(184) : Episode 2000: 平均奖励=0.6153, Actor损失=-0.6175, Critic损失=0.0006
[2025-11-07 10:04:21] Trainer.py(184) : Episode 2100: 平均奖励=0.6151, Actor损失=-0.6173, Critic损失=0.0005
[2025-11-07 10:05:10] Trainer.py(184) : Episode 2200: 平均奖励=0.6149, Actor损失=-0.6173, Critic损失=0.0005
[2025-11-07 10:05:58] Trainer.py(184) : Episode 2300: 平均奖励=0.6139, Actor损失=-0.6168, Critic损失=0.0005
[2025-11-07 10:06:46] Trainer.py(184) : Episode 2400: 平均奖励=0.6149, Actor损失=-0.6162, Critic损失=0.0005
[2025-11-07 10:07:34] Trainer.py(184) : Episode 2500: 平均奖励=0.6156, Actor损失=-0.6159, Critic损失=0.0005
[2025-11-07 10:08:27] Trainer.py(184) : Episode 2600: 平均奖励=0.6155, Actor损失=-0.6166, Critic损失=0.0005
[2025-11-07 10:09:24] Trainer.py(184) : Episode 2700: 平均奖励=0.6151, Actor损失=-0.6159, Critic损失=0.0005
[2025-11-07 10:10:16] Trainer.py(184) : Episode 2800: 平均奖励=0.6153, Actor损失=-0.6152, Critic损失=0.0005
[2025-11-07 10:11:03] Trainer.py(184) : Episode 2900: 平均奖励=0.6158, Actor损失=-0.6166, Critic损失=0.0005
[2025-11-07 10:11:50] Trainer.py(184) : Episode 3000: 平均奖励=0.6169, Actor损失=-0.6162, Critic损失=0.0005
[2025-11-07 10:11:50] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 10:11:50] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 10:11:50] Trainer.py(192) : Episode 3000: 保存了新的最佳模型，平均奖励=0.6169
[2025-11-07 10:12:37] Trainer.py(184) : Episode 3100: 平均奖励=0.6165, Actor损失=-0.6166, Critic损失=0.0004
[2025-11-07 10:13:26] Trainer.py(184) : Episode 3200: 平均奖励=0.6169, Actor损失=-0.6161, Critic损失=0.0004
[2025-11-07 10:14:17] Trainer.py(184) : Episode 3300: 平均奖励=0.6167, Actor损失=-0.6163, Critic损失=0.0004
[2025-11-07 10:15:07] Trainer.py(184) : Episode 3400: 平均奖励=0.6168, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:15:57] Trainer.py(184) : Episode 3500: 平均奖励=0.6169, Actor损失=-0.6162, Critic损失=0.0004
[2025-11-07 10:16:44] Trainer.py(184) : Episode 3600: 平均奖励=0.6174, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:16:44] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 10:16:44] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 10:16:44] Trainer.py(192) : Episode 3600: 保存了新的最佳模型，平均奖励=0.6174
[2025-11-07 10:17:32] Trainer.py(184) : Episode 3700: 平均奖励=0.6169, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:18:18] Trainer.py(184) : Episode 3800: 平均奖励=0.6171, Actor损失=-0.6161, Critic损失=0.0004
[2025-11-07 10:19:05] Trainer.py(184) : Episode 3900: 平均奖励=0.6166, Actor损失=-0.6164, Critic损失=0.0004
[2025-11-07 10:19:52] Trainer.py(184) : Episode 4000: 平均奖励=0.6168, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:20:39] Trainer.py(184) : Episode 4100: 平均奖励=0.6165, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:21:32] Trainer.py(184) : Episode 4200: 平均奖励=0.6163, Actor损失=-0.6168, Critic损失=0.0004
[2025-11-07 10:22:27] Trainer.py(184) : Episode 4300: 平均奖励=0.6163, Actor损失=-0.6165, Critic损失=0.0004
[2025-11-07 10:23:22] Trainer.py(184) : Episode 4400: 平均奖励=0.6162, Actor损失=-0.6164, Critic损失=0.0004
[2025-11-07 10:24:11] Trainer.py(184) : Episode 4500: 平均奖励=0.6171, Actor损失=-0.6164, Critic损失=0.0004
[2025-11-07 10:24:58] Trainer.py(184) : Episode 4600: 平均奖励=0.6173, Actor损失=-0.6179, Critic损失=0.0004
[2025-11-07 10:25:51] Trainer.py(184) : Episode 4700: 平均奖励=0.6169, Actor损失=-0.6170, Critic损失=0.0004
[2025-11-07 10:26:38] Trainer.py(184) : Episode 4800: 平均奖励=0.6166, Actor损失=-0.6170, Critic损失=0.0004
[2025-11-07 10:27:26] Trainer.py(184) : Episode 4900: 平均奖励=0.6171, Actor损失=-0.6163, Critic损失=0.0004
[2025-11-07 10:28:13] Trainer.py(184) : Episode 5000: 平均奖励=0.6176, Actor损失=-0.6170, Critic损失=0.0004
[2025-11-07 10:28:13] Trainer.py(304) : --- 保存最佳模型 (N=10) ---
[2025-11-07 10:28:13] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10_best.pth
[2025-11-07 10:28:13] Trainer.py(192) : Episode 5000: 保存了新的最佳模型，平均奖励=0.6176
[2025-11-07 10:28:13] Trainer.py(306) : --- 训练完成 (N=10) ---
[2025-11-07 10:28:13] Trainer.py(319) : 模型已保存到: ./result/20251107_094705_train_geo_csf/model_N10.pth
[2025-11-07 10:28:13] Trainer.py(202) :  *** 单阶段训练完成！ *** 
